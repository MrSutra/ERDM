{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rumor-model-twt1516.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z2RXTV-mQBun",
        "Z0khzDogXUAL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBqgas0b_eLJ"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, classification_report\n",
        "\n",
        "# if cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# randomness\n",
        "# seed = 0\n",
        "# torch.manual_seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# random.seed(seed)\n",
        "\n",
        "# torch.cuda.manual_seed_all(seed)\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQbCNpQN-TGc"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZDkUVD-Rgv"
      },
      "source": [
        "class TwitterDataset(Dataset):\n",
        "  def __init__(self, folder, hours):\n",
        "    self.folder = folder\n",
        "    self.hours = hours\n",
        "    self.embeddings = {}\n",
        "    self.ids = {}\n",
        "    self.labels = {}\n",
        "    self._labels = {}\n",
        "    self.eid_to_ids = {}\n",
        "    self.label_to_int = {'true':0, 'false':1, 'non-rumor':2, 'unverified':3}\n",
        "\n",
        "    self._load_labels()\n",
        "    self._load_embeddings()\n",
        "    self._get_norms()\n",
        "\n",
        "    # sanity check\n",
        "    assert len(self.embeddings) == len(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.embeddings)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "  def _load_embeddings(self):\n",
        "    \"\"\"Read embedings from files\"\"\"\n",
        "    files = os.listdir(self.folder)\n",
        "\n",
        "    for idx, file in enumerate(files):\n",
        "      # save embeddings and id\n",
        "      emb = np.load(self.folder + file)\n",
        "      emb = torch.from_numpy(emb).float()\n",
        "      if len(emb) == 0:\n",
        "        print('{} vacio'.format(file))\n",
        "        continue\n",
        "\n",
        "      # filter by date\n",
        "      emb = emb[emb[:, -1] <= (60.0 * self.hours)]\n",
        "      self.embeddings[idx] = emb\n",
        "\n",
        "      eid = file.split('.')[0]\n",
        "      self.ids[idx] = eid\n",
        "      self.labels[idx] = self._labels[eid]\n",
        "      self.eid_to_ids[eid] = idx\n",
        "\n",
        "  def _load_labels(self):\n",
        "    with open('label.txt') as file:\n",
        "      for line in file:\n",
        "        line = line.strip().split(':')\n",
        "        self._labels[line[1]] = torch.tensor([self.label_to_int[line[0]]])\n",
        "        #if line[0] in ['true','false']:\n",
        "        #  self._labels[line[1]] = torch.tensor([1]) if line[0]=='true' \\\n",
        "        #                                            else torch.tensor([0])\n",
        "                                      \n",
        "\n",
        "  def _get_norms(self):\n",
        "    features = torch.zeros((1,12))\n",
        "    for claim in self.embeddings.values():\n",
        "      features = torch.cat([features, claim[:,300:]], dim=0)\n",
        "      features = features[1:] # delete row zero\n",
        "    for id, claim in self.embeddings.items():\n",
        "      self.embeddings[id][:,300:] = (self.embeddings[id][:,300:] - torch.mean(features,dim=0))/torch.std(features, dim=0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx6WqwKTuz1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f57d653-c765-4ab4-fac4-2cfdbc855d46"
      },
      "source": [
        "# load dataloader\n",
        "dataset = TwitterDataset('embeddings/', 24.0)\n",
        "print(\"Data loaded\")\n",
        "\n",
        "# load ids\n",
        "import json\n",
        "ids = json.load('data_ids.json')\n",
        "train_ids, dev_ids, test_ids = ids['train'], ids['dev'], ids['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGKcXdZF-atb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSOr2nvFOlvt"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  \"\"\"Attention nn module that is responsible for computing the alignment scores.\"\"\"\n",
        "  def __init__(self, method, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.method = method\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Define attention layer\n",
        "    if self.method == 'general':\n",
        "      self.attention = nn.Linear(self.hidden_size * 2, self.hidden_size * 2)\n",
        "    elif self.method == 'concat':\n",
        "      self.attention = nn.Linear(self.hidden_size * 4, self.hidden_size * 2)\n",
        "      self.v = nn.Parameter(torch.FloatTensor(self.hidden_size * 2))\n",
        "\n",
        "  def forward(self, hidden, encoder_out):\n",
        "    '''\n",
        "      input:\n",
        "        hidden: [hidden_size]\n",
        "        encoder_out: [len-1, hidden_size]\n",
        "      output:\n",
        "        weights: (1, encoder_out)\n",
        "    '''\n",
        "    weights = torch.zeros((1, encoder_out.size(0)), device=device)\n",
        "    for i, out in enumerate(encoder_out):\n",
        "      weights[:,i] = self._score(hidden, out)\n",
        "    return F.softmax(weights, dim=1)\n",
        "\n",
        "\n",
        "  def _score(self, hidden, encoder_output):\n",
        "    \"\"\"Calculate the relevance of a particular rnn output in respect to the prediction.\"\"\"\n",
        "    if self.method == 'dot':\n",
        "      weight = hidden.dot(encoder_output)\n",
        "    elif self.method == 'general':\n",
        "      weight = self.attention(encoder_output)\n",
        "      weight = hidden.T.dot(weight)\n",
        "    elif self.method == 'concat':\n",
        "      weight = torch.tanh(self.attention(torch.cat((hidden, encoder_output),0)))\n",
        "      weight = self.v.T.dot(weight)\n",
        "    return weight\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, bi_d=True, dropout=0, num_layers=1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.bi_d = bi_d\n",
        "\n",
        "    self.bi_gru = nn.GRU(\n",
        "      input_size, \n",
        "      hidden_size, \n",
        "      bidirectional=bi_d,\n",
        "      dropout=dropout,\n",
        "      num_layers=num_layers\n",
        "    )\n",
        "\n",
        "  def forward(self, x, hidden=None):\n",
        "    hidden = torch.zeros((1+int(self.bi_d))*self.num_layers, x.size(1), self.hidden_size).to(device)\n",
        "    output, hidden = self.bi_gru(x, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "class RumorModel(nn.Module):\n",
        "  def __init__(self, input_size=300, hidden_size=32, bi_d=True, dropout=0.0, method='dot'):\n",
        "    super(RumorModel, self).__init__()\n",
        "    \n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bi_d = bi_d\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "      input_size,\n",
        "      hidden_size,\n",
        "      num_layers=1\n",
        "    )\n",
        "    self.attention = Attention(\n",
        "      method,\n",
        "      hidden_size\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.f1 = nn.Linear(hidden_size * 4, 32)\n",
        "    self.f2 = nn.Linear(32, 2)\n",
        "    self.activation = F.relu\n",
        "\n",
        "    self.user = nn.Linear(12, 32)\n",
        "    self.user2 = nn.Linear(32, 64)\n",
        "\n",
        "  def forward(self, x=None):\n",
        "    u = self.dropout(self.activation(self.user(x[:, 300:])))\n",
        "    u = self.activation(self.user2(u))\n",
        "    x = torch.cat((x[:, :300], u), 1)\n",
        "\n",
        "    # encoder\n",
        "    hidden = None\n",
        "    out, _ = self.encoder(x.unsqueeze(0), hidden)\n",
        "    h = out[0, 0, :]\n",
        "    hs = out[0, 1: ,:]\n",
        "\n",
        "    # attention module\n",
        "    align = self.attention(h, hs)\n",
        "    context = torch.bmm(align.unsqueeze(0), hs.unsqueeze(0))\n",
        "    out = torch.cat((context[0], h.unsqueeze(0)), 1)\n",
        "\n",
        "    # linear layer\n",
        "    out = self.activation(self.f1(out))\n",
        "    out = self.dropout(out)\n",
        "    out = self.f2(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqljgTHenWSi"
      },
      "source": [
        "def train(net, dataset, train_ids, criterion, optimizer):\n",
        "  total_loss = []\n",
        "\n",
        "  # start training\n",
        "  net.train()\n",
        "  for idx in train_ids:\n",
        "    x, y = dataset[idx]\n",
        "\n",
        "    # Get data to cuda if possible\n",
        "    x = x.to(device=device)\n",
        "    y = y.to(device=device)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    out = net(x)\n",
        "    loss = criterion(out, y)\n",
        "\n",
        "    # backward and opt step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # save loss\n",
        "    total_loss.append(loss.item())\n",
        "  \n",
        "  return np.mean(total_loss)\n",
        "\n",
        "def dev(net, data_dev, dev_ids, criterion):\n",
        "  total_loss = []\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for idx in dev_ids:\n",
        "      x, y = data_dev[idx]\n",
        "\n",
        "      # Get data to cuda if possible\n",
        "      x = x.to(device=device)\n",
        "      y = y.to(device=device)\n",
        "\n",
        "      # forward\n",
        "      out = net(x)\n",
        "      loss = criterion(out, y)\n",
        "\n",
        "      # save loss\n",
        "      total_loss.append(loss.item())\n",
        "  \n",
        "  return np.mean(total_loss)\n",
        "\n",
        "\n",
        "def test(net, data_test, test_ids, epoch):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for idx in test_ids:  \n",
        "      x, y = data_test[idx]\n",
        "\n",
        "      x = x.to(device=device)\n",
        "      y = y.to(device=device)\n",
        "\n",
        "      out = net(x)\n",
        "      _, y_hat = torch.max(out.data, 1)\n",
        "      y_true.append(y.item())\n",
        "      y_pred.append(y_hat.item())\n",
        "\n",
        "\n",
        "  return accuracy_score(y_true, y_pred),\n",
        "   recall_score(y_true, y_pred),\n",
        "   precision_score(y_true, y_pred),\n",
        "   f1_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y3ynZksU3UR"
      },
      "source": [
        "# Main routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkXzE2DtUx24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c7211d-dc17-405a-fb36-c20d5a6aed88"
      },
      "source": [
        "def main(args):\n",
        "\n",
        "  # parse args\n",
        "  epochs = args['epoch']\n",
        "  learning_rate = args['lr']\n",
        "  input_size = args['input_size']\n",
        "  hidden_size = args['hidden_size']\n",
        "  att_method = args['att_method']\n",
        "  dropout = args['dropout']\n",
        "  weight_decay = args['weight_decay']\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "  # define model\n",
        "  net = RumorModel(\n",
        "    input_size,\n",
        "    hidden_size,\n",
        "    dropout=dropout\n",
        "  ).to(device)\n",
        "  print(\"-Model created\")\n",
        "\n",
        "  # loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(\n",
        "    net.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay\n",
        "  )\n",
        "\n",
        "  loss = 0\n",
        "  train_losses = []\n",
        "  dev_losses = []\n",
        "  all_f1 = []\n",
        "  all_acc = []\n",
        "  all_rec = []\n",
        "  all_pre = []\n",
        "  \n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"----------Epochs {}----------\".format(epoch))\n",
        "    random.shuffle(train_ids)\n",
        "    start = time.time()\n",
        "\n",
        "    print('-Training')\n",
        "    loss_t = train(net, dataset, train_ids, criterion, optimizer)\n",
        "    loss_d = dev(net, dataset, dev_ids, criterion)\n",
        "    print('train loss:{:.4f}, dev loss:{:.4f}\\n-Validation'.format(loss_t, loss_d))\n",
        "\n",
        "    acc, rec, pre, f1 = test(net, dataset, test_ids, epoch)\n",
        "    print('acc:{:.4f}, rec:{:.4f}, pre:{:.4f}, f1:{:.4f}'.format(acc, rec, pre, f1))\n",
        "    print('Time epochs:{:.4f}'.format(time.time()-start, loss))\n",
        "\n",
        "    train_losses.append(loss_t)\n",
        "    dev_losses.append(loss_d)\n",
        "    all_f1.append(f1)\n",
        "    all_acc.append(acc)\n",
        "    all_rec.append(rec)\n",
        "    all_pre.append(pre)\n",
        "  torch.save(net, 'net.pt')\n",
        "\n",
        "  return train_losses, dev_losses, all_f1, all_acc, all_rec, all_pre\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  args = {}\n",
        "  args['epoch'] = 100\n",
        "  args['lr'] = 0.0001\n",
        "  args['input_size'] = 364\n",
        "  args['hidden_size'] = 32\n",
        "  args['att_method'] = 'dot'\n",
        "  args['dropout'] = 0.2\n",
        "  args['weight_decay'] = 0\n",
        "\n",
        "  trainloss, devloss, allf1, allacc, allrec, allpre = main(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}